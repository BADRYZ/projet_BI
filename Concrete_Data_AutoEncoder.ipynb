{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Import the required libraries:**\n",
        "\n",
        "\n",
        "> #  Think of these as tools we'll use to process data, build our model, and analyze results.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TtXm2MOExJxO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "75uVHbZLpkIM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import FunctionTransformer, PowerTransformer, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define helper functions:**\n",
        "\n",
        "\n",
        "\n",
        "> # These functions help us create missing data and calculate how well our imputation works.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VCL3s_9nxr76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def missing_method(raw_data, mechanism='mcar', method='random', missing_threshold=0.2, random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "    data = raw_data.copy()\n",
        "    rows, cols = data.shape\n",
        "    t = missing_threshold\n",
        "\n",
        "    if mechanism == 'mcar':\n",
        "        v = np.random.uniform(size=(rows, cols))\n",
        "        if method == 'uniform':\n",
        "            mask = v <= t\n",
        "        elif method == 'random':\n",
        "            c = np.zeros(cols, dtype=bool)\n",
        "            c[np.random.choice(cols, cols // 2, replace=False)] = True\n",
        "            mask = (v <= t) & c[np.newaxis, :]\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown method: {method}\")\n",
        "    elif mechanism == 'mnar':\n",
        "        sample_cols = np.random.choice(cols, 2, replace=False)\n",
        "        m1, m2 = np.median(data[:, sample_cols], axis=0)\n",
        "        v = np.random.uniform(size=(rows, cols))\n",
        "        m = (data[:, sample_cols[0]] <= m1) & (data[:, sample_cols[1]] >= m2)\n",
        "        mask = v <= t\n",
        "        if method == 'uniform':\n",
        "            mask &= m[:, np.newaxis]\n",
        "        elif method == 'random':\n",
        "            c = np.zeros(cols, dtype=bool)\n",
        "            c[np.random.choice(cols, cols // 2, replace=False)] = True\n",
        "            mask &= m[:, np.newaxis] & c[np.newaxis, :]\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown method: {method}\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown mechanism: {mechanism}\")\n",
        "\n",
        "    data[mask] = np.nan\n",
        "    return data, mask"
      ],
      "metadata": {
        "id": "m1eR4lRvpt5N"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imputation_rmse(clean_data, imputed_data, missing_mask):\n",
        "    if clean_data.shape != imputed_data.shape:\n",
        "        raise ValueError(\"Clean and imputed datasets must have the same shape\")\n",
        "\n",
        "    missing_mask = missing_mask.astype(bool)\n",
        "\n",
        "    if not np.any(missing_mask):\n",
        "        raise ValueError(\"The missing_mask does not contain any True values.\")\n",
        "\n",
        "    errors = clean_data[missing_mask] - imputed_data[missing_mask]\n",
        "    rmse = np.sqrt(np.mean(errors**2))\n",
        "\n",
        "    metrics = {\n",
        "        'rmse': rmse,\n",
        "        'mae': np.mean(np.abs(errors)),\n",
        "        'total_missing': np.sum(missing_mask),\n",
        "        'missing_percentage': np.sum(missing_mask) / missing_mask.size * 100,\n",
        "        'min_error': np.min(errors) if errors.size > 0 else None,\n",
        "        'max_error': np.max(errors) if errors.size > 0 else None,\n",
        "        'std_error': np.std(errors) if errors.size > 0 else None\n",
        "    }\n",
        "\n",
        "    return rmse, metrics"
      ],
      "metadata": {
        "id": "HOgJuti8p4zQ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load dataset**\n",
        "\n",
        "\n",
        "\n",
        "> # We are reading a file containing health data into a table format.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BH7WLjuGyKFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"elikplim/concrete-compressive-strength-data-set\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDJ40yacp6OM",
        "outputId": "813821cd-dab8-4195-8dc6-21a0536c18da"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/elikplim/concrete-compressive-strength-data-set/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/root/.cache/kagglehub/datasets/elikplim/concrete-compressive-strength-data-set/versions/1/concrete_data.csv\""
      ],
      "metadata": {
        "id": "jUEft9r_qJAX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split into train and test**\n",
        "\n",
        "\n",
        "\n",
        "> # We divide our data into two parts: one for learning and one for testing if the learning worked.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2FkPVQwuyZcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "diabetes = pd.read_csv(data_path)"
      ],
      "metadata": {
        "id": "kidfrpk0qLzv"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test\n",
        "train_set, test_set = train_test_split(diabetes, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "3xFgdF2mqPqP"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing pipeline**\n",
        "\n",
        "\n",
        "\n",
        "> # A pipeline is like a recipe. Here, we're setting up steps to clean and prepare our data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hwGfZcNHzMBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "])"
      ],
      "metadata": {
        "id": "NC8EQewMqeOh"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fit preprocessing on train and transform train and test**\n",
        "\n",
        "\n",
        "> # We \"teach\" the pipeline using the training data and then apply it to both train and test data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Iu2ivKpgznnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit preprocessing on train and transform train and test\n",
        "preprocessed_train_set = preprocessing_pipeline.fit_transform(train_set)\n",
        "preprocessed_test_set = preprocessing_pipeline.transform(test_set)"
      ],
      "metadata": {
        "id": "4hzJ6p0Uqfd_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Verify preprocessed data**\n",
        "\n",
        "> #  We check if everything looks okay after preprocessing, especially for missing values.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ym0frBsBz0_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify preprocessed data\n",
        "if np.isnan(preprocessed_train_set).any():\n",
        "    print(\"Warning: Training data contains NaNs after preprocessing!\")\n",
        "if np.isnan(preprocessed_test_set).any():\n",
        "    print(\"Warning: Test data contains NaNs after preprocessing!\")"
      ],
      "metadata": {
        "id": "ts9aejuauzkf"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generate missing data in test set**\n",
        "\n",
        "> # Here, we artificially create missing data in our test set to simulate real-world scenarios.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rtzwCUWC0ATl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate missing data in test set\n",
        "missing_test_set, missing_mask = missing_method(preprocessed_test_set, mechanism='mcar', method='random', missing_threshold=0.4)"
      ],
      "metadata": {
        "id": "e2nOGGA9qjfQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NaNs in missing data\n",
        "if np.isnan(missing_test_set).any():\n",
        "    print(\"Warning: Missing test set contains NaNs!\")\n",
        "if np.isinf(missing_test_set).any():\n",
        "    print(\"Warning: Missing test set contains infinite values!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EQkiUVHu4hB",
        "outputId": "0382ca5c-b416-433e-d8e0-276ca2e0d339"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Missing test set contains NaNs!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Autoencoder model**\n",
        "\n",
        "\n",
        "> # This is our  model that will learn how to fill in missing values.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_VOkzx6A0NUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Autoencoder model\n",
        "input_dim = preprocessed_train_set.shape[1]\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoder = Dense(64, activation=\"relu\")(input_layer)\n",
        "encoder = Dense(32, activation=\"relu\")(encoder)\n",
        "\n",
        "decoder = Dense(64, activation=\"relu\")(encoder)\n",
        "decoder = Dense(input_dim, activation=\"linear\")(decoder)\n",
        "\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "WFTcBZg3qnQF"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train autoencoder**\n",
        "# We show our model the training data multiple times until it learns to process it well.\n"
      ],
      "metadata": {
        "id": "XEryOiN60m9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train autoencoder\n",
        "history = autoencoder.fit(preprocessed_train_set, preprocessed_train_set, epochs=50, batch_size=32, shuffle=True, validation_split=0.2, verbose=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7beRZlaCr6GU",
        "outputId": "05dfaad3-a562-41fb-b0aa-e28ba36a679d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "Epoch 2/50\n",
            "Epoch 3/50\n",
            "Epoch 4/50\n",
            "Epoch 5/50\n",
            "Epoch 6/50\n",
            "Epoch 7/50\n",
            "Epoch 8/50\n",
            "Epoch 9/50\n",
            "Epoch 10/50\n",
            "Epoch 11/50\n",
            "Epoch 12/50\n",
            "Epoch 13/50\n",
            "Epoch 14/50\n",
            "Epoch 15/50\n",
            "Epoch 16/50\n",
            "Epoch 17/50\n",
            "Epoch 18/50\n",
            "Epoch 19/50\n",
            "Epoch 20/50\n",
            "Epoch 21/50\n",
            "Epoch 22/50\n",
            "Epoch 23/50\n",
            "Epoch 24/50\n",
            "Epoch 25/50\n",
            "Epoch 26/50\n",
            "Epoch 27/50\n",
            "Epoch 28/50\n",
            "Epoch 29/50\n",
            "Epoch 30/50\n",
            "Epoch 31/50\n",
            "Epoch 32/50\n",
            "Epoch 33/50\n",
            "Epoch 34/50\n",
            "Epoch 35/50\n",
            "Epoch 36/50\n",
            "Epoch 37/50\n",
            "Epoch 38/50\n",
            "Epoch 39/50\n",
            "Epoch 40/50\n",
            "Epoch 41/50\n",
            "Epoch 42/50\n",
            "Epoch 43/50\n",
            "Epoch 44/50\n",
            "Epoch 45/50\n",
            "Epoch 46/50\n",
            "Epoch 47/50\n",
            "Epoch 48/50\n",
            "Epoch 49/50\n",
            "Epoch 50/50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check training history**\n",
        "# After training, we check how well the model did during learning."
      ],
      "metadata": {
        "id": "ACVU9O8N0uGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check training history\n",
        "print(\"Training Loss:\", history.history['loss'][-1])\n",
        "print(\"Validation Loss:\", history.history['val_loss'][-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXI0sB49tfLw",
        "outputId": "fb1aff5b-9f02-4a20-bd9f-8380ed18d09e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.28529223054647446\n",
            "Validation Loss: 0.39678732864558697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Impute missing values**\n",
        "# We use our trained model to guess the missing values in the test set."
      ],
      "metadata": {
        "id": "zJKgn9dc000X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values\n",
        "imputed_test_set = autoencoder.predict(missing_test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--OTtJktsPFU",
        "outputId": "3f5d9b16-304e-4234-dfa3-23cd41caac71"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Handle NaN values in the imputed data**\n",
        "# If the guesses from the model have NaNs, we replace them with the average values of the dataset.\n"
      ],
      "metadata": {
        "id": "XNTbxYSZ0618"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle NaN values in the imputed data\n",
        "if np.isnan(imputed_test_set).any():\n",
        "    print(\"Warning: Imputed data contains NaNs. Applying fallback imputation.\")\n",
        "    imputed_test_set = np.where(np.isnan(imputed_test_set), np.nanmean(imputed_test_set, axis=0), imputed_test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdS2rj8xtx2J",
        "outputId": "3d0e7fb6-646c-4755-e1d6-6ff883b02c93"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Imputed data contains NaNs. Applying fallback imputation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate performance**\n",
        "# We calculate how close the guesses are to the original values using RMSE and other metrics.\n"
      ],
      "metadata": {
        "id": "spGuuVmf0-rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate performance\n",
        "clean_test_set = preprocessed_test_set\n",
        "rmse, metrics = imputation_rmse(clean_test_set, imputed_test_set, missing_mask)"
      ],
      "metadata": {
        "id": "kBpS2hGUsRr9"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RMSE:\", rmse)\n",
        "print(\"Metrics:\", metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffpE2TCAsWtD",
        "outputId": "c65255b3-422e-434b-8f8d-4274053a481b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 1.0574767758678276\n",
            "Metrics: {'rmse': 1.0574767758678276, 'mae': 0.8049457653153316, 'total_missing': 331, 'missing_percentage': 17.853290183387273, 'min_error': -1.9296161723675702, 'max_error': 5.3609976045019385, 'std_error': 1.0539464881148253}\n"
          ]
        }
      ]
    }
  ]
}