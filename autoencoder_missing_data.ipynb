{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# **Autoencoder for Missing Data Imputation**\n", "This notebook demonstrates how to preprocess, train, and evaluate an Autoencoder for handling missing data. We will cover the following steps:\n\n", "1. **Data Loading and Preprocessing**\n", "2. **Data Transformation and Encoding**\n", "3. **Missing Data Simulation**\n", "4. **Autoencoder Implementation**\n", "5. **Model Training with Early Stopping**\n", "6. **Evaluation and Imputation Analysis**\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## **1\ufe0f\u20e3 Data Loading**\n", "In this section, we load the dataset and prepare it for processing. We stratify the data to ensure balanced splits for training and testing."]}, {"cell_type": "code", "metadata": {}, "source": ["import os\n", "import pandas as pd\n", "import numpy as np\n", "from sklearn.model_selection import train_test_split\n", "\n", "def load_data(path):\n", "    df = pd.read_csv(path, sep=';')\n", "    df['Age_cat'] = pd.cut(df['age'], bins=[10, 20, 30, 40, 50, 60, np.inf], labels=[1, 2, 3, 4, 5, 6])\n", "    train_set, test_set = train_test_split(df, test_size=0.1, stratify=df['Age_cat'], random_state=42)\n", "    for set_ in (train_set, test_set):\n", "        set_.drop(['Age_cat', 'y'], axis=1, inplace=True)\n", "    return train_set, test_set\n", "\n", "# Example usage:\n", "# path = 'path/to/bank-full.csv'\n", "# train_set, test_set = load_data(path)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## **2\ufe0f\u20e3 Data Transformation and Encoding**\n", "In this section, we apply transformations to the data, including log transformations, power transformations, and one-hot encoding."]}, {"cell_type": "code", "metadata": {}, "source": ["from sklearn.preprocessing import PowerTransformer, OneHotEncoder, StandardScaler, OrdinalEncoder\n", "from sklearn.compose import ColumnTransformer\n", "\n", "preprocessor = ColumnTransformer([\n", "    ('log', 'passthrough', ['duration']),\n", "    ('pt', PowerTransformer(), ['balance']),\n", "    ('scaler', StandardScaler(), ['age', 'campaign', 'pdays', 'previous']),\n", "    ('ordinal', OrdinalEncoder(), ['education', 'month']),\n", "    ('nominal', OneHotEncoder(), ['job', 'marital', 'default', 'housing', 'loan', 'contact', 'poutcome'])\n", "])\n", "\n", "# Example usage:\n", "# preprocessed_train_set = preprocessor.fit_transform(train_set)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## **3\ufe0f\u20e3 Missing Data Simulation**\n", "We simulate missing data using MCAR (Missing Completely At Random) and MNAR (Missing Not At Random) mechanisms. This step introduces controlled missingness into the dataset."]}, {"cell_type": "code", "metadata": {}, "source": ["def apply_missingness(data, mechanism='mcar', missing_threshold=0.2, random_state=42):\n", "    np.random.seed(random_state)\n", "    data = data.copy()\n", "    mask = np.random.rand(*data.shape) < missing_threshold\n", "    data[mask] = np.nan\n", "    return data, mask\n", "\n", "# Example usage:\n", "# corrupted_data, missing_mask = apply_missingness(preprocessed_train_set, 'mcar', 0.2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## **4\ufe0f\u20e3 Autoencoder Implementation**\n", "We define a simple Autoencoder using PyTorch to reconstruct missing data. The encoder reduces dimensionality, and the decoder reconstructs the input."]}, {"cell_type": "code", "metadata": {}, "source": ["import torch\n", "import torch.nn as nn\n", "\n", "class AutoEncoder(nn.Module):\n", "    def __init__(self, input_dim):\n", "        super(AutoEncoder, self).__init__()\n", "        self.encoder = nn.Sequential(\n", "            nn.Linear(input_dim, 128),\n", "            nn.ReLU()\n", "        )\n", "        self.decoder = nn.Sequential(\n", "            nn.Linear(128, input_dim)\n", "        )\n", "    \n", "    def forward(self, x):\n", "        encoded = self.encoder(x)\n", "        decoded = self.decoder(encoded)\n", "        return decoded"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## **5\ufe0f\u20e3 Training the Autoencoder**\n", "We train the Autoencoder with early stopping. If the validation loss does not improve for a set number of epochs, training stops early."]}, {"cell_type": "code", "metadata": {}, "source": ["def train_autoencoder(model, train_loader, epochs=50, learning_rate=1e-3):\n", "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n", "    loss_fn = nn.MSELoss()\n", "    for epoch in range(epochs):\n", "        total_loss = 0\n", "        for batch_data, in train_loader:\n", "            optimizer.zero_grad()\n", "            output = model(batch_data)\n", "            loss = loss_fn(output, batch_data)\n", "            loss.backward()\n", "            optimizer.step()\n", "            total_loss += loss.item()\n", "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}')\n", "\n", "# Example usage:\n", "# model = AutoEncoder(input_dim=preprocessed_train_set.shape[1])\n", "# train_autoencoder(model, train_loader)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.8"}}, "nbformat": 4, "nbformat_minor": 2}